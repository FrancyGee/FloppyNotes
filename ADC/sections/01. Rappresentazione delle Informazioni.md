
---
title: Rappresentazione delle Informazioni
date: 2022-12-15
tags: ADC, binary, numbers, conversions, errors
difficulty: ðŸŸ¢
---

<h1  style="text-align: center;"> 01 Rappresentazione delle Informazioni </h1>

Il calcolatore utilizza il **sistema numerico binario (o base 2)** per rappresentare le informazioni, questo sistema utilizza solo due simboli $0$ e $1$  che fisicamente rappresentano lo stato degli elettroni che viaggiano nel nostro computer, zero significa che non passa (ha una carica molto bassa) nessun elettrone, uno significa che passa un elettrone.

Con questi soli due simboli noi possiamo comunicare con il calcolatore e rappresentare numeri, caratteri, frasi, immagini, video, audio ecc...

# Rappresentazione dei Numeri

Per rappresentare i numeri con il sistema binario ci basta convertire dalla **base 10** che utilizziamo noi alla **base 2** che il calcolatore interpreta, per farlo eseguiamo questa semplice operazione:
- Prendiamo un numero in base 10, esempio $14$.
- Dividiamo per $2$ il numero, e teniamo conto del resto, il risultato andrÃ  diviso nuovamente per due e salveremo nuovamente il resto e cosÃ¬ via.
- Quando la divisione da risultato $0$ ci fermiamo e mettiamo in ordine tutti i resti ottenuti dal primo all'ultimo.
- il numero che otterremo sarÃ  l'equivalente binario del numero decimale di partenza.

$$
\begin{align}
& 14 : 2 = 7  \quad \text{resto } 0 \\
& 7 : 2 = 3  \quad \text{resto } 1 \\
& 3 : 2 = 1  \quad \text{resto } 1 \\
& 1 : 2 = 0  \quad \text{resto } 1 \\
\end{align}
$$
Il 14 in base 2 Ã¨ $1110$

Se volessimo trasformare un numero binario in decimale dovremmo invece procedere in questo modo, prendiamo il numero $1011$, ogni cifra ha una posizione:

| $p_{n-1}$ | $p2$ | $p_{1}$ | $p_{0}$ |
| ------- | ---- | ------- | ------- |
| 1       | 0    | 1       | 1       |

Dove:
- $n$ Ã¨ il numero di cifre del numero binario.
- $p_{n-1}$ Ã¨ la **cifra piÃ¹ significativa**.
- $p_{0}$ Ã¨ la **cifra meno significativa.**


Per riportare il numero da binario a decimale dovremo:
- Elevare 2 per il valore della posizione.
- Moltiplicare $2^{px}$ per il la cifra in posizione $px$
- Sommare tutti i prodotti di ogni posizione:

$$
1011 = 1\cdot 2^{3} + 0\cdot 2^{2} + 1\cdot 2^{1} + 1\cdot 2^{0} = 8 + 0  
+ 2 + 1 = 11 
$$

> [!NOTE] NOTA
> Se la cifra meno significativa Ã¨ $1$ allora il numero Ã¨ dispari, altrimenti Ã¨ pari

Abbiamo utilizzato numeri positivi intero, in matematica abbiamo anche i numeri negativi, i numeri razionali, irrazionali, periodici ecc... che richiedono alcune regole specifiche per essere rappresentati.



# Numeri Interi

Con i numeri interi possiamo rappresentare:
- *numeri senza segno* (**Unsigned**).
- *numeri positivi*.
- *numeri negativi*.

## Interi senza segno

Per gli interi senza segno la rappresentazione va da $0$ a $2^{n}-1$.
**ESEMPIO:**
con un $1$ byte possiamo rappresentare 256 numeri da $0$ a $255$.

## Interi relativi

Per i numeri **interi relativi (interi con segno)** abbiamo bisogno  di rappresentare:
- *Il valore assoluto del numero.*
- *il segno del numero*.

Per fare ciÃ² esistono tecniche differenti:
- **modulo e segno**
- **completamento a due**
- **eccesso-k**

#### **modulo e segno**

Il bit piÃ¹ significativo rappresenta il segno, gli altri $n-1$ rappresentano il numero.
**ESEMPIO:**
- $0111 =  +7$
- $1111 = -7$

> [!NOTE] NOTA
> Questa rappresentazione ci *"ruba"* un bit e quindi ci limita a rappresentare con $n$ bit solo $2^{n-1}-1$ numeri.

### **complemento a due**

Nel complemento a due abbiamo una rappresentazione che ci permette di avere con $n$ bit 
$2^{n-1}$ cifre negative e $2^{n-1}-1$ cifre positive.

Nel complemento a due il numero negativo Ã¨ l'inverso del numero positivo a cui viene poi sommato $1$.
Questo tipo di rappresentazione agevola l'utilizzo di algoritmi di somma e sottrazione per esempio con 4 bit abbiamo:
![[Pasted image 20221209164901.png|center]]




### **eccesso-k**
per codificare in **eccesso k** un numero intero, si somma al numero il valore k e si converte il valore ottenuto in binario su n bit.

Con questo metodo noi possiamo rappresentare tutti i numeri interi da $-k$ a $2^{n-1}-k$.
Per esempio con 4 bit abbiamo:
![[Pasted image 20221209165057.png|center]]


# Numeri Reali

I numeri reali necessitano oltre che la rappresentazione del **numero**, del **valore assoluto** e del **segno**, anche della **parte decimale.**

Inoltre i con gli $\mathbb{R}$ noi abbiamo anche i numeri periodici e irrazionali che per limiti di memoria il calcolatore non puÃ² rappresentare.
In questi casi si parlerÃ  quindi di metodi di approssimazione del numero.

Per rappresentare i numeri reali possiamo usufruire di due metodi:
- **Fixed Point**, rappresentazione a virgola fissa.
- **Floating Point**, rappresentazione a virgola mobile.


## Fixed Point

La rappresentazione in **virgola fissa** ci permette di rappresentare numeri con la virgola di cui sappiamo giÃ  di quante cifre intere e di quante cifre decimali Ã¨ composto.

Infatti nella rappresentazione a virgola fissa abbiamo con $n$ bit:
- 1 bit per il segno
- $i$ bit per la parte intera
- $f$ bit per la parte decimale
dove $f = n-i-1$.

Prendiamo per esempio n = 8, i=5, f=2 avremo che:
![[ADC-fixed_point_rappresentation.excalidraw|center]]

## Floating Point (IEEE 754)

I floating point (definito nello **standard IEEE 754**) sono un metodo per rappresentare numeri reali con un certo numero di bit.
Essi sono costituiti da una parte intera ed una parte decimale.

Per rappresentare un numero con i floating point abbiamo bisogno di:
- Un bit per il **segno**.
- $m$ bit per **mantissa**. 
- $n$ bit per l'**esponente**. 

 I numeri in virgola mobile si possono rappresentare con la seguente formula : 
$$(-1)^s \times (1 + m ) \times 2^{n}$$
 dove:
 - $s$ Ã¨ il bit del segno 
 - $m$ Ã¨ la mantissa 

## Arrotondamenti
Quando lavoriamo con numeri reali c'Ã¨ sempre una certa approssimazione, poichÃ© i numeri reali non possono essere rappresentati per intero in un sistema binario.
Ad esempio nella rappresentazione a virgola mobile IEEE 754, con 32 bit abbiamo:
- 1 bit per il segno.
- 23 bit per la mantissa.
- 8 bit per l'esponente.

Questo significa che non possiamo rappresentare tutti i numeri reali ottenendo come risultato una **approssimazione** del numero reale.


## Double e Float

nella `libc` e nella IEEE 754 vi sono due tipi di floating point

| **TIPO** | **DIMENSIONE** | **SEGNO** | **ESPONENTE** | **MANTISSA** |
| -------- | -------------- | --------- | ------------- | ------------ |
| `float`  | 32 bit  (4 byte)       | 1 bit     | 8 bit         | 23 bit       |
| `double` | 64 bit   (8 byte)       | 1 bit     | 11 bit        | 52 bit       |

Esempio di codice per vedere la dimensione di `float` e `double` in `C`:

```c
int main()
{
	printf("%lu\n", sizeof(float));  // 4 byte
	printf("%lu\n", sizeof(double)); // 8 byte
}
```

# Controllo degli Errori

Durante la trasmissione dei dati Ã¨ possibile avere delle perdite di informazione, prendiamo per esempio due modem in comunicazione durante la fase analogica potrebbero avere dei disturbi che possono interferire nella trasmissione dell'informazione (es: **rumore**, **interferenze**).

![[ADC-perdita_informazione_modem.excalidraw|center|5000]]

> [!info] **ANALOGICO E DIGITALE**
> La differenza fondamentale tra i due tipi di segnale Ã¨ che mentre **nei segnali analogici l'informazione Ã¨ contenuta nella "forma" stessa del segnale (elettricitÃ , elettroni,...), nei segnali digitali l'informazione da elaborare Ã¨ codificata in serie di simboli binari**. 

Per capire di essere certi di aver ricevuto la completezza dell'informazione esistono algoritmi e codici da aggiungere all'informazione per capire se il segnale Ã¨ stato disturbato o meno, tra questi abbiamo:
- **codici di paritÃ **
- **CRC**
- **Codice di Hamming**


## Codice di ParitÃ 

Esistono due tipi di codici di paritÃ :
- **paritÃ  pari**
- **paritÃ  dispari**

Conto quanti bit sono impostati a 1 nell'informazione.

Se prendo un'informazione con dispari bit impostati a 1:
- Con la paritÃ  pari devo aggiungere un bit a 1 (**in posizione meno significativa**) per far si che i bit a 1 diventino pari.
- Con la paritÃ  dispari devo aggiungere un bit a 0 (**in posizione meno significativa**) per far si che i bit a 1 rimangano pari.

Processo inverso se i bit dell'informazione sono pari.
Questo metodo ormai Ã¨ **deprecato** ma comunque *good to know*.

## CRC (Cyclic Redundancy Check)

Va inserire nei bit meno significativi che saranno il **resto di una divisione** fatta all'informazione dal mittente, il ricevente eseguirÃ  la medesima divisione e se:
- i risultati sono uguali **NON** si sono perse informazioni
- i risultati sono diverse si sono perse informazioni


## Distanza (codice) di Hamming

Il **codice di Hamming** Ã¨ un codice che permette di aggiungere un certo numero di bit ai bit di dati in modo da comporre parole con **distanza 3**, in grado di rilevare e correggere errori su un singolo bit. Il numero di bit da aggiungere aumenta all'aumentare del numero dei bit di dati [abaluth](http://informatica.abaluth.com/il-computer/le-informazioni/codice-correttore-codice-di-hamming/).

> [!abstract] DEFINIZIONE DISTANZA
> La distanza tra due stringhe di lunghezza uguale $s_{1}, s_{2}$ Ã¨ il numero di bit che differiscono tra le due stringhe, **esempio:**
> $s_{1} = 0\underline{1}001\underline{0}1\underline{1}$
> $s_{2} = 0\underline{0}001\underline{1}1\underline{0}$
> hanno **distanza 3**, due stringhe di informazioni uguali invece avranno distanza $0$.


## Codice di Espansione

I codici di espansioni servono a comprimere la dimensione di un'informazione senza perderne il contenuto attraverso determinati algoritmi.

Prendiamo per esempio l'alfabeto italiano, esso Ã¨ composto da 21 caratteri quindi per rappresentarlo abbiamo bisogno di $2^{5}$ bit, se perÃ² suddividiamo le lettere dell'alfabeto in:
- vocali (5 caratteri)
- consonanti (16 caratteri)

Avremo che serviranno:
- 3 bit per le vocali 
- 5 bit per le consonanti

In questo modo quando scriveremo una frase la dimensione sarÃ  $3*n_{vocali} + 5*m_{consonanti}$ e non $5*\text{lunghezza frase}$, esempio
*"Ciao mondo"* normalmente sarebbe grossa (spazi esclusi) 45 bit mentre con la versione espansa sarebbe $3\times5 + 4\times5 = 35$ bit. 


## Ridondanza

La ridondanza Ã¨ il **rapporto tra le $n$ combinazioni totali di un'informazione e le $n$ combinazioni utilizzate**:
$$
\frac{n\text{ combinazioni totali}}{n \text{ combinazioni utilizzate}}
$$

Un codice minimale ha una ridondanza fra $1$ e $2$, se Ã¨ maggiore di 2 il codice Ã¨ ridondante.

## Entropia informazione

[definizione](https://it.wikipedia.org/wiki/Entropia_(teoria_dell%27informazione)


# Flashcards

Come Ã¨ suddiviso il **floating point**?
?
**32 bit** divisi in:
- **1 bit** per il segno.
- **23 bit** per la mantissa.
- **8 bit** per l'esponente.

Cos'Ã¨ la ridondanza?:: $$\frac{n\text{ combinazioni totali}}{n \text{ combinazioni utilizzate}}$$



# | [[02. Circuiti Logici e Sequenziali| Next â†’]]








